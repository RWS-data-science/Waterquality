{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'geopandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c2099e58e533>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgeopandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'geopandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import gc\n",
    "from itertools import product\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "import folium\n",
    "\n",
    "from shapely.geometry import Point\n",
    "from fiona.crs import from_epsg\n",
    "\n",
    "from pyproj import Proj, transform\n",
    "import altair.vegalite.v1 as alt\n",
    "\n",
    "import networkx as nx\n",
    "#import pandas_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blue = '#1f77b4'\n",
    "orange = '#ff7f0e'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataDF = pd.read_csv('/media/ondrej/500/ds_data/rws_datalab/data_tot.csv', encoding='latin_1', index_col=0)\n",
    "\n",
    "to_drop = []\n",
    "for col in dataDF.columns:\n",
    "    if dataDF[col].nunique() < 2:\n",
    "        to_drop.append(col)\n",
    "\n",
    "#for c in ['DATUM', 'TIJD']:\n",
    "#    to_drop.append(c)\n",
    "print('Dropping {}'.format(to_drop))\n",
    "dataDF.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "#dataDF['datetime'] = pd.to_datetime(dataDF.apply(lambda x: x['DATUM'] + ' ' + x['TIJD'][1:3] + ':' + x['TIJD'][3:], axis=1), \n",
    "#                                    format='%Y-%m-%d %H:%M')\n",
    "dataDF['datetime'] = pd.to_datetime(dataDF['DATUM'], format='%Y-%m-%d')\n",
    "dataDF.drop(['DATUMTIJDWAARDE', 'DATUM', 'TIJD'], inplace=True, axis=1)\n",
    "\n",
    "to_drop = ['TIJD', 'DOM', 'WNS', 'PLT:X', 'PLT:Y', 'SYS', 'TYP', 'EXTCODE', 'BRON']\n",
    "dataDF.drop(to_drop, axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "# don't drop duplicates - it doesnt seem to do anything for this dataset\n",
    "drop = False\n",
    "\n",
    "if drop:\n",
    "    print('Dropping duplicates.')\n",
    "    print(dataDF.shape)\n",
    "    dataDF = dataDF.fillna('missing_data')\n",
    "    print(dataDF.head())\n",
    "    dataDF = dataDF.drop_duplicates()\n",
    "    dataDF = dataDF.replace('missing_data', np.nan)\n",
    "    print(dataDF.head())\n",
    "    print(dataDF.shape)\n",
    "\n",
    "for col in dataDF.columns:\n",
    "    if dataDF[col].dtype == 'O' and dataDF[col].nunique() < 0.1*len(dataDF):\n",
    "        dataDF[col] = dataDF[col].astype('category')\n",
    "\n",
    "dataDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgbDF = pd.read_excel('loc_sgb.xls')\n",
    "\n",
    "print(sgbDF.shape)\n",
    "sgbDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataDF = pd.merge(dataDF,sgbDF.rename(columns={'mpn_mpnident':'LOC'})[['LOC','SGB']], on='LOC')\n",
    "\n",
    "dataDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pandas_profiling.ProfileReport(dataDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic stats about data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(12,12))\n",
    "\n",
    "dataDF.groupby('LOC').size().plot.hist(bins=40)\n",
    "\n",
    "ax.set_xlabel('Number of measurements per location', fontsize=18)\n",
    "ax.set_ylabel('Number of locations', fontsize=18)\n",
    "ax.set_title('Distribution of the number of measurements among the {} locations'.format(dataDF['LOC'].nunique()), fontsize=20)\n",
    "\n",
    "med = dataDF.groupby('LOC').size().median()\n",
    "ax.axvline(x=med, color=orange, linewidth=5, linestyle='dashed')\n",
    "ax.text(0.8,0.95, 'Median: {0:d}'.format(int(med)), horizontalalignment='center',\n",
    "        verticalalignment='center', transform=ax.transAxes, fontdict={'size':25, 'color':orange})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inProj = Proj(init='epsg:28992')\n",
    "outProj = Proj(init='epsg:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapa = folium.Map([52., 6.],\n",
    "                  zoom_start=7,\n",
    "                  tiles='cartodbpositron')\n",
    "\n",
    "measCounts = folium.FeatureGroup(name='Measurement counts')\n",
    "measKinds = folium.FeatureGroup(name='Parameters')\n",
    "\n",
    "for loc,group in dataDF.groupby('LOCOMS'):\n",
    "    if group['X_RD'].nunique() != 1:\n",
    "        continue\n",
    "    x = group['X_RD'].unique()[0]\n",
    "    y = group['Y_RD'].unique()[0]\n",
    "    lon,lat = transform(inProj, outProj, x,y)\n",
    "    \n",
    "    myHtml = folium.Html('''<h4>Location {}</h4>\n",
    "                           <ul style=\"list-style-type:disc\">\n",
    "                          <li>Total measurements: {}</li>\n",
    "                          <li>Oldest: {}</li>\n",
    "                          <li>Newest: {}</li>\n",
    "                            </ul>\n",
    "                            '''.format(loc,len(group), group['datetime'].min().strftime('%d %b %Y'), \n",
    "                                       group['datetime'].max().strftime('%d %b %Y')), script=True)\n",
    "    \n",
    "    popup = folium.Popup(myHtml, max_width=2650)\n",
    "\n",
    "    folium.Circle([lat,lon], radius=0.2*len(group), fill=True, fill_color=blue, fill_opacity=0.6, \n",
    "                  color=blue, line_opacity=0.8, popup=popup).add_to(measCounts)\n",
    "\n",
    "    valCnts = group['PAROMS'].value_counts()\n",
    "    valCnts = valCnts[valCnts > 0]\n",
    "    \n",
    "    maxPars = ', '.join(list(valCnts[valCnts == valCnts.max()].index))\n",
    "    minPars = ', '.join(list(valCnts[valCnts == valCnts.min()].index))\n",
    "    #myHtml = folium.Html('''<h4>Location {0}</h4>\n",
    "    #                       <ul style=\"list-style-type:disc\">\n",
    "    #                      <li>Measured parameters {1}</li>\n",
    "    #                      <li>Most measurements {2} ({3})</li>\n",
    "    #                      <li>Fewest measurements {4} ({5})</li>\n",
    "    #                        </ul>\n",
    "    #                        '''.format(loc, group['PAR'].nunique(), valCnts.max(), maxPars,\n",
    "    #                                   valCnts.min(), minPars), script=True)\n",
    "    myHtml = folium.Html('''<h4>Location {0}</h4>\n",
    "                           <ul style=\"list-style-type:disc\">\n",
    "                          <li>Measured parameters: {1}</li>\n",
    "                          <li>Most measurements: {2} </li>\n",
    "                          <li>Fewest measurements: {3} </li>\n",
    "                            </ul>\n",
    "                            '''.format(loc, group['PAR'].nunique(), valCnts.max(), \n",
    "                                       valCnts.min()), script=True)\n",
    "        \n",
    "    popup = folium.Popup(myHtml, max_width=2650)\n",
    "    \n",
    "    folium.Circle([lat,lon], radius=20*group['PAR'].nunique(), color=orange, \n",
    "                  fill=True, fill_color=orange, fill_opacity=0.6, \n",
    "                  line_opacity=0.8, popup=popup).add_to(measKinds)\n",
    "\n",
    "measCounts.add_to(mapa)    \n",
    "measKinds.add_to(mapa)\n",
    "\n",
    "folium.LayerControl().add_to(mapa)\n",
    "\n",
    "mapa.save('water_quality.html', close_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(12,12))\n",
    "\n",
    "dataDF.groupby('PAR').size().plot.hist(bins=40)\n",
    "\n",
    "ax.set_xlabel('Number of measurements per parameter', fontsize=18)\n",
    "ax.set_ylabel('Number of measured qunatities', fontsize=18)\n",
    "ax.set_title('Distribution of the number of measurements among the {} parameters'.format(dataDF['PAR'].nunique()), fontsize=20)\n",
    "\n",
    "med = dataDF.groupby('PAR').size().median()\n",
    "ax.axvline(x=med, color=orange, linewidth=5, linestyle='dashed')\n",
    "ax.text(0.7,0.95, 'Median: {0:d}'.format(int(med)), horizontalalignment='left', \n",
    "        verticalalignment='center', transform=ax.transAxes, fontdict={'size':25, 'color':orange})\n",
    "\n",
    "med = dataDF.groupby(['PAR','LOC']).size().median()\n",
    "#ax.axvline(x=med, color=orange, linewidth=5, linestyle='dashed')\n",
    "ax.text(0.7,0.91, 'Median per location: {0:d}'.format(int(med)), horizontalalignment='left', \n",
    "        verticalalignment='center', transform=ax.transAxes, fontdict={'size':15, 'color':'grey'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "dataDF.groupby('LOC')['PAR'].nunique().plot.hist(bins=30,ax=ax)\n",
    "\n",
    "ax.set_xlabel('Number of parameters measured per location', fontsize=18)\n",
    "ax.set_ylabel('Number of locations', fontsize=18)\n",
    "ax.set_title('Distribution of the number of parameters measured at each location'.format(dataDF['PAR'].nunique()), fontsize=20)\n",
    "\n",
    "med = dataDF.groupby('LOC')['PAR'].nunique().median()\n",
    "ax.axvline(x=med, color=orange, linewidth=5, linestyle='dashed')\n",
    "ax.text(0.8,0.95, 'Median: {0:d}'.format(int(med)), horizontalalignment='center', \n",
    "        verticalalignment='center', transform=ax.transAxes, fontdict={'size':25, 'color':orange})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapa = folium.Map([52., 6.],\n",
    "                  zoom_start=7,\n",
    "                  tiles='cartodbpositron')\n",
    "\n",
    "for loc,group in dataDF.groupby('LOCOMS'):\n",
    "    if group['X_RD'].nunique() != 1:\n",
    "        continue\n",
    "    x = group['X_RD'].unique()[0]\n",
    "    y = group['Y_RD'].unique()[0]\n",
    "    lon,lat = transform(inProj, outProj, x,y)\n",
    "    \n",
    "    myHtml = folium.Html('''<h4>Location {0}</h4>\n",
    "                           <ul style=\"list-style-type:disc\">\n",
    "                          <li>Measured parameters: {1}</li>\n",
    "                          <li>Most measurements: {2} ({3})</li>\n",
    "                          <li>Fewest measurements: {4} ({5})</li>\n",
    "                            </ul>\n",
    "                            '''.format(loc, group['PAR'].nunique(), valCnts.max(), maxPars,\n",
    "                                       valCnts.min(), minPars), script=True)\n",
    "       \n",
    "    popup = folium.Popup(myHtml, max_width=2650)\n",
    "    \n",
    "    folium.Circle([lat,lon], radius=20*group['PAR'].nunique(), color=orange, \n",
    "                  fill=True, fill_color=orange, fill_opacity=0.6, \n",
    "                  line_opacity=0.8, popup=popup).add_to(mapa)\n",
    "\n",
    "mapa.save('basic_map_measured_parameters.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking out nutrients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nutrients = [\"chlorofyl-a\",\"nitraat\",\"nitriet\", \n",
    "                    \"stikstof\",\"orthofosfaat\",\"ammonium\",\n",
    "                    \"totaal fosfaat\", \"Zwevende stof\", \n",
    "                    \"Temperatuur\", \"zuurstof\", \"Doorzicht\",\n",
    "                    \"Zuurgraad\",\"Kjeldahl stikstof\"]\n",
    "\n",
    "chlorDF = dataDF[['datetime', 'LOCOMS', 'WAARDE', 'PAROMS']].copy()\n",
    "chlorDF = chlorDF[chlorDF['PAROMS'].map(lambda x: x in nutrients)]\n",
    "\n",
    "chlorDF.head()\n",
    "\n",
    "#del dataDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featDF = pd.pivot_table(chlorDF, index='datetime', columns=['PAROMS','LOCOMS']).groupby(pd.Grouper(freq='1W')).mean().stack()\n",
    "featDF.columns = featDF.columns.droplevel()\n",
    "\n",
    "# get rid of the outliers\n",
    "featDF[featDF < featDF.quantile(0.005)] = np.nan\n",
    "featDF[featDF > featDF.quantile(0.995)] = np.nan\n",
    "\n",
    "featDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chDF = featDF['chlorofyl-a'].reset_index()\n",
    "chDF = pd.pivot_table(chDF, index='datetime', columns='LOCOMS', values='chlorofyl-a')\n",
    "\n",
    "chDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(20,10))\n",
    "\n",
    "for col in chDF.isnull().sum().sort_values()[:6].index:\n",
    "    chDF[col].dropna().plot(ax=ax, label=col, marker='o')\n",
    "    \n",
    "ax.legend()\n",
    "ax.set_yscale('log')\n",
    "#ax.set_ylim(-1,200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pal = sns.color_palette('hls', len(nutrients)).as_hex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapa = folium.Map([52., 6.],\n",
    "                  zoom_start=7,\n",
    "                  tiles='cartodbpositron')\n",
    "\n",
    "for cnt,nut in enumerate(nutrients):\n",
    "    measCounts = folium.FeatureGroup(name='{} measurements'.format(nut))\n",
    "    #measKinds = folium.FeatureGroup(name='Parameters')\n",
    "\n",
    "    for loc,group in dataDF[dataDF['PAROMS'] == nut].groupby('LOCOMS'):\n",
    "        if len(group) ==0:\n",
    "            continue\n",
    "            \n",
    "        if group['X_RD'].nunique() != 1:\n",
    "            continue\n",
    "        x = group['X_RD'].unique()[0]\n",
    "        y = group['Y_RD'].unique()[0]\n",
    "        lon,lat = transform(inProj, outProj, x,y)\n",
    "        \n",
    "        if group['EHD'].nunique() > 1:\n",
    "            targ_unit = [u for u in group['EHD'].unique() if 'l' in u][0]\n",
    "            tmpDF = group.loc[group['EHD'] == targ_unit, ['datetime','LOCOMS', 'WAARDE']].rename(columns={'datetime':'Date', \n",
    "                                                                                                                         'LOCOMS':'Location','WAARDE': nut}).copy()\n",
    "        else:\n",
    "            tmpDF = group[['datetime','LOCOMS', 'WAARDE']].rename(columns={'datetime':'Date', \n",
    "                                                                           'LOCOMS':'Location','WAARDE': nut}).copy()\n",
    "        \n",
    "        \n",
    "        myChart = alt.Chart(tmpDF, title='{} at {}'.format(nut.capitalize(),loc)).mark_line(point=True).encode(\n",
    "                                                                                              x='Date:T',\n",
    "                                                                                              y=alt.Y('{}:Q'.format(nut), \n",
    "                                                                                                      axis=alt.Axis(title='{} ({})'.format(nut, group['EHD'].unique()[0]))),\n",
    "                                                                                              #y='{}:Q'.format(nut),\n",
    "                                                                                            color='Location:N').configure(color=pal[cnt])\n",
    "        \n",
    "        vega = folium.VegaLite(myChart, width='100%', height='100%')\n",
    "\n",
    "        popup = folium.Popup(max_width=600).add_child(vega)\n",
    "\n",
    "        folium.Circle([lat,lon], radius=30*len(group), fill=True, fill_color=pal[cnt], fill_opacity=0.6, \n",
    "                      color=pal[cnt], line_opacity=0.8, popup=popup).add_to(measCounts)\n",
    "\n",
    "\n",
    "    measCounts.add_to(mapa)    \n",
    "    #measKinds.add_to(mapa)\n",
    "    \n",
    "folium.LayerControl().add_to(mapa)\n",
    "\n",
    "mapa.save('nutrient_map_timeseries.html')#'nutrients_basic_map.html', close_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataDF.loc[grp.index[2:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(20,20))\n",
    "sns.heatmap(featDF.corr(), annot=True, ax=ax, square=True, cmap='RdBu_r', vmax=1., vmin=-1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "incl_cols = []\n",
    "\n",
    "cnt = 0\n",
    "for ind,row in featDF.corr().iterrows():\n",
    "    vals = list(row.values)\n",
    "    _ = vals.pop(cnt)\n",
    "    vals = np.array(vals)\n",
    "    if np.any(np.abs(vals) > 0.65):\n",
    "        incl_cols.append(ind)\n",
    "    \n",
    "    cnt += 1\n",
    "\n",
    "incl_cols = list(set(incl_cols))\n",
    "incl_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def corrfunc(x, y, ax, **kws):\n",
    "    r, _ = stats.pearsonr(x, y)\n",
    "    ax.annotate(\"r = {:.2f}\".format(r),\n",
    "                xy=(.1, .9), fontsize=20, xycoords=ax.transAxes)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(4*len(featDF.columns), 4*len(featDF.columns)), nrows=len(featDF.columns), ncols=len(featDF.columns))\n",
    "\n",
    "for i,j in product(range(len(featDF.columns)), range(len(featDF.columns))):\n",
    "    if i == j:\n",
    "        featDF[featDF.columns[i]].plot.hist(ax=ax[i,j], bins=15, alpha=0.6)\n",
    "    elif i < j:\n",
    "        ax[i,j].set_visible(False)\n",
    "    else:\n",
    "        featDF.plot.scatter(ax=ax[i,j], x=featDF.columns[i], y=featDF.columns[j], alpha=0.2)\n",
    "        locDF = featDF[[featDF.columns[i],featDF.columns[j]]].dropna().copy()\n",
    "        corrfunc(locDF[locDF.columns[0]], locDF[locDF.columns[1]], ax[i,j])\n",
    "    \n",
    "    sns.despine(ax=ax[i,j])\n",
    "    \n",
    "    if i == len(featDF.columns)-1:\n",
    "        ax[i,j].set_xlabel(featDF.columns[j])\n",
    "    else:\n",
    "        ax[i,j].set_xlabel('')\n",
    "        plt.setp(ax[i,j].get_xticklabels(), visible=False)\n",
    "        \n",
    "    if j == 0:\n",
    "        ax[i,j].set_ylabel(featDF.columns[i])\n",
    "    else:\n",
    "        ax[i,j].set_ylabel('')\n",
    "        plt.setp(ax[i,j].get_yticklabels(), visible=False)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chlorDF = pd.pivot_table(data=dataDF.loc[dataDF['PAROMS'] == 'chlorofyl-a'], columns='LOCOMS', index='datetime', values='WAARDE')\n",
    "chlorDF = chlorDF.resample('1m').mean()\n",
    "\n",
    "print(chlorDF.shape)\n",
    "chlorDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(25,10))\n",
    "\n",
    "#sns.heatmap(chlorDF.T.notnull(), cmap='Blues')\n",
    "\n",
    "tmpDF = chlorDF.copy()\n",
    "tmpDF.index = tmpDF.index.map(lambda x: x.strftime('%d%b%Y'))\n",
    "\n",
    "sns.heatmap(tmpDF.T.notnull(), xticklabels=4, yticklabels=False, cbar=False) #, cmap=['#1f77b4', '#ff7f0e'])\n",
    "#sns.heatmap(tmpDF.T, xticklabels=4, vmin=0, vmax=20, yticklabels=False, cbar=False)\n",
    "\n",
    "ax.set_ylabel('Locations', fontsize=20)\n",
    "ax.set_title('Cholorfyl-a measurement availability', fontsize=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Location-based correlations\n",
    "\n",
    "In the first case, we plot the correlation of a given parameter across all locations it's measured at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "# the smalest number of measurements required to make an image that have no upper or lower limit\n",
    "cutoff = 0.6\n",
    "meas_cutoff = dataDF[dataDF['BGC'].isnull()].groupby('PAROMS').size().quantile(cutoff)\n",
    "\n",
    "totPar = dataDF['PAROMS'].nunique()\n",
    "\n",
    "cnt = 0\n",
    "for par, grp in dataDF.groupby('PAROMS'):\n",
    "    cnt +=1 \n",
    "    sys.stdout.write('\\rDoing {} ({}/{})'.format(par,cnt,totPar))\n",
    "    \n",
    "    if cnt < 130:\n",
    "        continue \n",
    "    \n",
    "    if cnt > 260:\n",
    "        break\n",
    "        \n",
    "    # filter out upper and lower limits\n",
    "    group = grp[grp['BGC'].isnull()].copy()\n",
    "\n",
    "    if group['EHD'].nunique() > 1:\n",
    "        targ_unit = [u for u in group['EHD'].unique() if 'l' in u][0]\n",
    "        group = group.loc[group['EHD'] == targ_unit]             \n",
    "\n",
    "    # skip if not measured in a lot of locations\n",
    "    if group['LOCOMS'].nunique() < 10:\n",
    "        continue\n",
    "        \n",
    "    # ignore if not enough measurements\n",
    "    if len(group) < meas_cutoff:\n",
    "        continue\n",
    "        \n",
    "    # if there is a lot of nans, don't continue\n",
    "    #if corrMat.isnull().sum().sum() > 0.95*corrMat.size:\n",
    "    #    continue\n",
    "                \n",
    "    tmpDF = pd.pivot_table(data=group, index='datetime', columns='LOCOMS', values='WAARDE')\n",
    "    corrMat = tmpDF.corr(min_periods=12)\n",
    "\n",
    "    corrMat = corrMat.dropna(how='all')\n",
    "    corrMat = corrMat.T.dropna(how='all').T\n",
    "\n",
    "    mask = np.zeros_like(corrMat, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask,1)] = True\n",
    "\n",
    "    fig,ax = plt.subplots(figsize=(3*corrMat.shape[0], corrMat.shape[1]), \n",
    "                          ncols=3)\n",
    "\n",
    "    sns.heatmap(corrMat, ax=ax[0], mask=mask, annot=True, vmin=-1, vmax=1, cmap='RdBu', \n",
    "                square=True, fmt='.2f', annot_kws={'fontsize':16}, cbar=False)\n",
    "    ax[0].set_title('Parameter: {} ({} measurements, daily)'.format(par, len(group)), fontsize=25)\n",
    "    ax[0].set_xticklabels(ax[0].get_xmajorticklabels(), fontdict={'fontsize': 22})\n",
    "    ax[0].set_yticklabels(ax[0].get_ymajorticklabels(), fontdict={'fontsize': 22})\n",
    "    \n",
    "    tmpDF = pd.pivot_table(data=group, index='datetime', columns='LOCOMS', values='WAARDE').groupby(pd.Grouper(freq='1W')).mean()\n",
    "    corrMat = tmpDF.corr(min_periods=12)\n",
    "    \n",
    "    corrMat = corrMat.dropna(how='all')\n",
    "    corrMat = corrMat.T.dropna(how='all').T\n",
    "\n",
    "    mask = np.zeros_like(corrMat, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask,1)] = True\n",
    "\n",
    "    sns.heatmap(corrMat, ax=ax[1], mask=mask, annot=True, vmin=-1, vmax=1, cmap='RdBu', \n",
    "                square=True, fmt='.2f', annot_kws={'fontsize':16}, cbar=False)\n",
    "    ax[1].set_title('Parameter: {} ({} measurements, weekly)'.format(par, len(group)), fontsize=25)\n",
    "    ax[1].set_xticklabels(ax[1].get_xmajorticklabels(), fontdict={'fontsize': 22})\n",
    "    ax[1].set_yticklabels(ax[1].get_ymajorticklabels(), fontdict={'fontsize': 22})\n",
    "\n",
    "    tmpDF = pd.pivot_table(data=group, index='datetime', columns='LOCOMS', values='WAARDE').groupby(pd.Grouper(freq='1m')).mean()\n",
    "    corrMat = tmpDF.corr(min_periods=12)\n",
    "    \n",
    "    corrMat = corrMat.dropna(how='all')\n",
    "    corrMat = corrMat.T.dropna(how='all').T\n",
    "\n",
    "    mask = np.zeros_like(corrMat, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask,1)] = True\n",
    "\n",
    "    sns.heatmap(corrMat, ax=ax[2], mask=mask, annot=True, vmin=-1, vmax=1, cmap='RdBu', \n",
    "                square=True, fmt='.2f', annot_kws={'fontsize':16}, cbar=False)\n",
    "    ax[2].set_title('Parameter: {} ({} measurements, montly)'.format(par, len(group)), fontsize=25)\n",
    "    ax[2].set_xticklabels(ax[2].get_xmajorticklabels(), fontdict={'fontsize': 22})\n",
    "    ax[2].set_yticklabels(ax[2].get_ymajorticklabels(), fontdict={'fontsize': 22})\n",
    "\n",
    "    fig.savefig('loc_corrs/location_corr_{}.svg'.format(par), bbox_inches='tight', pad_inches=0.2, format='svg')\n",
    "    \n",
    "    fig.clf()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    #if cnt > 35:\n",
    "    #    break\n",
    "\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import seaborn as sns\n",
    "\n",
    "# the smalest number of measurements required to make an image that have no upper or lower limit\n",
    "cutoff = 0.6\n",
    "meas_cutoff = dataDF[dataDF['BGC'].isnull()].groupby('PAROMS').size().quantile(cutoff)\n",
    "\n",
    "totLocs = dataDF['LOCOMS'].nunique()\n",
    "\n",
    "cnt = 0\n",
    "for par, grp in dataDF.groupby('LOCOMS'):\n",
    "    cnt +=1 \n",
    "    sys.stdout.write('\\rDoing {} ({}/{})'.format(par,cnt,totLocs))\n",
    "            \n",
    "    # filter out upper and lower limits\n",
    "    group = grp[grp['BGC'].isnull()].copy()\n",
    "                        \n",
    "    \n",
    "    tmpDF = pd.pivot_table(data=group, index='datetime', columns='PAROMS', values='WAARDE')\n",
    "    corrMat = tmpDF.corr(min_periods=12)\n",
    "\n",
    "    corrMat = corrMat.dropna(how='all')\n",
    "    corrMat = corrMat.T.dropna(how='all').T\n",
    "\n",
    "    np.fill_diagonal(corrMat.values, np.nan)\n",
    "\n",
    "    corrMat = corrMat[corrMat.abs() > 0.9].dropna(how='all').T\n",
    "    corrMat = corrMat[corrMat.abs() > 0.9].dropna(how='all').T\n",
    "\n",
    "    np.fill_diagonal(corrMat.values, 1.)\n",
    "\n",
    "    mask = np.zeros_like(corrMat, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask,1)] = True\n",
    "    \n",
    "    if corrMat.shape[0] == 0:\n",
    "        continue\n",
    "        \n",
    "    #fig,ax = plt.subplots(figsize=(int(1.5*corrMat.shape[0]), int(0.5*corrMat.shape[1])), \n",
    "    #                      ncols=3)\n",
    "    fig,ax = plt.subplots(figsize=(corrMat.shape[0], 0.5*corrMat.shape[1]))\n",
    "\n",
    "    sns.heatmap(corrMat, ax=ax, mask=mask, annot=True, vmin=-1, vmax=1, cmap='RdBu', \n",
    "                square=True, fmt='.2f', annot_kws={'fontsize':13}, cbar=False)\n",
    "    ax.set_title('Location: {} ({} measurements, daily)'.format(par, len(group)), fontsize=25)\n",
    "    ax.set_xticklabels(ax.get_xmajorticklabels(), fontdict={'fontsize': 22})\n",
    "    ax.set_yticklabels(ax.get_ymajorticklabels(), fontdict={'fontsize': 22})\n",
    "    \n",
    "    '''\n",
    "    tmpDF = pd.pivot_table(data=group, index='datetime', columns='PAROMS', values='WAARDE').groupby(pd.Grouper(freq='1W')).mean()\n",
    "    corrMat = tmpDF.corr(min_periods=12)\n",
    "    \n",
    "    np.fill_diagonal(corrMat.values, np.nan)\n",
    "\n",
    "    corrMat = corrMat[corrMat.abs() > 0.9].dropna(how='all').T\n",
    "    corrMat = corrMat[corrMat.abs() > 0.9].dropna(how='all').T\n",
    "\n",
    "    np.fill_diagonal(corrMat.values, 1.)\n",
    "\n",
    "    mask = np.zeros_like(corrMat, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask,1)] = True\n",
    "\n",
    "    sns.heatmap(corrMat, ax=ax[1], mask=mask, annot=True, vmin=-1, vmax=1, cmap='RdBu', \n",
    "                square=True, fmt='.2f', annot_kws={'fontsize':16}, cbar=False)\n",
    "    ax[1].set_title('Location: {} ({} measurements, weekly)'.format(par, len(group)), fontsize=25)\n",
    "    ax[1].set_xticklabels(ax[1].get_xmajorticklabels(), fontdict={'fontsize': 22})\n",
    "    ax[1].set_yticklabels(ax[1].get_ymajorticklabels(), fontdict={'fontsize': 22})\n",
    "\n",
    "    tmpDF = pd.pivot_table(data=group, index='datetime', columns='PAROMS', values='WAARDE').groupby(pd.Grouper(freq='1m')).mean()\n",
    "    corrMat = tmpDF.corr(min_periods=12)\n",
    "    \n",
    "    np.fill_diagonal(corrMat.values, np.nan)\n",
    "\n",
    "    corrMat = corrMat[corrMat.abs() > 0.9].dropna(how='all').T\n",
    "    corrMat = corrMat[corrMat.abs() > 0.9].dropna(how='all').T\n",
    "\n",
    "    np.fill_diagonal(corrMat.values, 1.)\n",
    "\n",
    "    mask = np.zeros_like(corrMat, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask,1)] = True\n",
    "\n",
    "    sns.heatmap(corrMat, ax=ax[2], mask=mask, annot=True, vmin=-1, vmax=1, cmap='RdBu', \n",
    "                square=True, fmt='.2f', annot_kws={'fontsize':16}, cbar=False)\n",
    "    ax[2].set_title('Location: {} ({} measurements, monthly)'.format(par, len(group)), fontsize=25)\n",
    "    ax[2].set_xticklabels(ax[2].get_xmajorticklabels(), fontdict={'fontsize': 22})\n",
    "    ax[2].set_yticklabels(ax[2].get_ymajorticklabels(), fontdict={'fontsize': 22})\n",
    "    '''\n",
    "    \n",
    "    fig.savefig('par_corrs/parameter_corr_{}.svg'.format(par), bbox_inches='tight', pad_inches=0.2, format='svg')\n",
    "    \n",
    "    fig.clf()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(24,6), ncols=4)\n",
    "\n",
    "#corrs = [-0.9, -0.3, 0., 0.5, 0.95]\n",
    "corrs = [0., 0.5, 0.9, 0.95]\n",
    "\n",
    "for i in range(4):\n",
    "    m = [0., 0.]\n",
    "    s = [1., 1.]\n",
    "\n",
    "    corr = corrs[i]        # correlation\n",
    "    covs = [[s[0]**2          , s[0]*s[1]*corr], \n",
    "            [s[0]*s[1]*corr,           s[1]**2]] \n",
    "\n",
    "    fake = np.random.multivariate_normal(m,covs, 1000).T\n",
    "\n",
    "    #fig,ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "    ax[i].scatter(fake[0],fake[1], alpha=0.6)\n",
    "    #ax[i].set_title('{0:.2f}'.format(corr), fontsize=40)\n",
    "    \n",
    "    ax[i].get_xaxis().set_visible(False)\n",
    "    ax[i].get_yaxis().set_visible(False)\n",
    "\n",
    "    ax[i].text(0.5,0.9, '{0:.2f}'.format(corr), horizontalalignment='center',\n",
    "            verticalalignment='center', transform=ax[i].transAxes, fontdict={'size':30},\n",
    "            bbox=dict(facecolor='white', alpha=0.9, edgecolor='black', boxstyle='round'))\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "fig.savefig('correlation_examples.png', bbox_inches='tight', pad_inches=0.2, \n",
    "            dpi=300, transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corrLim = 0.9\n",
    "totLocs = dataDF['LOCOMS'].nunique()\n",
    "\n",
    "corrDF = pd.DataFrame(columns=['parameters', 'uncorrelated'])\n",
    "\n",
    "cnt = 0\n",
    "for par, grp in dataDF.groupby('LOCOMS'):\n",
    "    cnt +=1 \n",
    "    sys.stdout.write('\\rDoing {} ({}/{})'.format(par,cnt,totLocs))\n",
    "    \n",
    "    tmp_dict = {'parameters': grp['PAROMS'].nunique()}\n",
    "    \n",
    "    # filter out upper and lower limits\n",
    "    group = grp[grp['BGC'].isnull()].copy()\n",
    "    \n",
    "    if group['EHD'].nunique() > 1:\n",
    "        targ_unit = [u for u in group['EHD'].unique() if 'l' in u][0]\n",
    "        group = group.loc[group['EHD'] == targ_unit]             \n",
    "    \n",
    "    tmpDF = pd.pivot_table(data=group, index='datetime', columns='PAROMS', values='WAARDE')\n",
    "    corrMat = tmpDF.corr(min_periods=12)\n",
    "\n",
    "    corrMat = corrMat.dropna(how='all')\n",
    "    corrMat = corrMat.T.dropna(how='all').T\n",
    "\n",
    "    np.fill_diagonal(corrMat.values, np.nan)\n",
    "\n",
    "    corrMat = corrMat[corrMat.abs() > corrLim].dropna(how='all').T\n",
    "    corrMat = corrMat[corrMat.abs() > corrLim].dropna(how='all').T\n",
    "\n",
    "    np.fill_diagonal(corrMat.values, 1.)\n",
    "\n",
    "    mask = np.zeros_like(corrMat, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask,1)] = True\n",
    "    \n",
    "    if corrMat.shape[0] == 0:\n",
    "        continue\n",
    "    \n",
    "    g = nx.Graph()\n",
    "    \n",
    "    for r in corrMat.index:\n",
    "        if r not in g.nodes:\n",
    "            g.add_node(r)\n",
    "            \n",
    "        for c in corrMat.columns:\n",
    "            if r == c:\n",
    "                continue\n",
    "            \n",
    "            if c not in g.nodes:\n",
    "                g.add_node(c)\n",
    "            \n",
    "            if np.abs(corrMat.loc[r,c]) >= corrLim:\n",
    "                g.add_edge(r,c)\n",
    "    \n",
    "    tmp_dict['uncorrelated'] = tmp_dict['parameters'] - corrMat.shape[0] + nx.number_connected_components(g)\n",
    "    corrDF.loc[par] = tmp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corrDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corrDF.loc['Terschelling 4 km uit de kust']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(corrDF['parameters'] - corrDF['uncorrelated']).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "upperDF = pd.DataFrame(index=dataDF['PAROMS'].unique(), columns=['measurements', 'unique', 'upperFrac'])\n",
    "\n",
    "cnt = 0\n",
    "for par, grp in dataDF.groupby('PAROMS'):\n",
    "    cnt +=1 \n",
    "    sys.stdout.write('\\rDoing {} ({}/{})'.format(par,cnt,dataDF['PAROMS'].nunique()))\n",
    "    \n",
    "    tmpDF = grp.copy()\n",
    "    tmpDF['val'] = tmpDF.apply(lambda x: '<{}'.format(x['WAARDE']) if x['BGC'] == '<' else str(x['WAARDE']), axis=1)\n",
    "    \n",
    "    tmpDict = {'unique': tmpDF['val'].nunique(), 'upperFrac':tmpDF['val'].map(lambda x: '<' in x).sum()/len(tmpDF), \n",
    "               'measurements':len(grp)}\n",
    "    \n",
    "    upperDF.loc[par] = tmpDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "upperDF.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.pairplot(upperDF, size=5, diag_kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial clustering from temporal correlations\n",
    "\n",
    "Our data contains 115 locations, where a combined 532 parameters are being measured. Let's look at basic similarity - for each parameter, we create a complete correlation matrix (daily, weekly, monthly). Then we average the pariwise correlations and cluster the results. finally, we draw the data on the map to see whether we get some kind of a spatial corelation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataDF = dataDF[dataDF['X_RD'].notnull()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "locations = dataDF['LOCOMS'].unique()\n",
    "\n",
    "corrMats = {'day':np.zeros((len(locations), len(locations))),\n",
    "            'week':np.zeros((len(locations), len(locations))),\n",
    "            'month':np.zeros((len(locations), len(locations)))}\n",
    "\n",
    "no_corrs = {'day':np.zeros((len(locations), len(locations))),\n",
    "            'week':np.zeros((len(locations), len(locations))),\n",
    "            'month':np.zeros((len(locations), len(locations)))}\n",
    "\n",
    "freq_dict = {'day': '1D', 'week': '1W', 'month':'1m'}\n",
    "\n",
    "cnt = 0\n",
    "for par, group in dataDF.groupby('PAROMS'):\n",
    "    cnt += 1\n",
    "    sys.stdout.write('\\rDoing {} ({}/{})'.format(par,cnt,dataDF['PAROMS'].nunique()))\n",
    "    \n",
    "    if group['EHD'].nunique() > 1:\n",
    "        targ_unit = list(group['EHD'].value_counts().index)[0]\n",
    "        grp = group[group['EHD'] == targ_unit].copy()\n",
    "    else:\n",
    "        grp = group.copy()\n",
    "        \n",
    "    for period in ['day', 'week', 'month']:\n",
    "        tmpDF = pd.pivot_table(grp, index='datetime', columns=['LOCOMS'], values='WAARDE').groupby(pd.Grouper(freq=freq_dict[period])).mean()\n",
    "        tmpDF = tmpDF.corr(min_periods=12) # with roughly one measurement a month, they should have at least a year of overlaps\n",
    "        tmpDF = tmpDF.reindex(index=locations, columns=locations)\n",
    "\n",
    "        no_corrs[period] = no_corrs[period] + tmpDF.notnull().applymap(int)\n",
    "\n",
    "        tmpDF.fillna(0, inplace=True)\n",
    "        tmpDF = tmpDF.abs()\n",
    "        \n",
    "        corrMats[period] = corrMats[period] + tmpDF.values\n",
    "    \n",
    "for period in corrMats:\n",
    "    corrMats[period] = corrMats[period]/no_corrs[period]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** use weighted averages to get the mean correlation (weighted by the number of overlapping points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "locations = dataDF['LOCOMS'].unique()\n",
    "parameters = dataDF['PAROMS'].unique()\n",
    "\n",
    "corrMats = {'day':np.zeros((len(locations), len(locations), len(parameters))),\n",
    "            'week':np.zeros((len(locations), len(locations), len(parameters))),\n",
    "            'month':np.zeros((len(locations), len(locations), len(parameters)))}\n",
    "\n",
    "corrWeigths = {'day':np.zeros((len(locations), len(locations), len(parameters))),\n",
    "               'week':np.zeros((len(locations), len(locations), len(parameters))),\n",
    "               'month':np.zeros((len(locations), len(locations), len(parameters)))}\n",
    "\n",
    "freq_dict = {'day': '1D', 'week': '1W', 'month':'1m'}\n",
    "\n",
    "cnt = 0\n",
    "for par, grp in dataDF.groupby('PAROMS'):\n",
    "    cnt += 1\n",
    "    sys.stdout.write('\\rDoing {} ({}/{})'.format(par,cnt,dataDF['PAROMS'].nunique()))\n",
    "    \n",
    "    for period in ['day', 'week', 'month']:\n",
    "        hlpDF = pd.pivot_table(grp, index='datetime', columns=['LOCOMS'], values='WAARDE').groupby(pd.Grouper(freq=freq_dict[period])).mean()\n",
    "        tmpDF = hlpDF.corr(min_periods=12) # with roughly one measurement a month, they should have at least a year of overlaps\n",
    "        tmpDF = tmpDF.reindex(index=locations, columns=locations)\n",
    "\n",
    "        #no_corrs[period] = no_corrs[period] + tmpDF.notnull().applymap(int)\n",
    "\n",
    "        tmpDF.fillna(0, inplace=True)\n",
    "        tmpDF = tmpDF.abs()\n",
    "        \n",
    "        corrMats[period][:,:,cnt-1] = tmpDF.values\n",
    "        \n",
    "        # calcualte weights - more values used to calculate the correlation means more confidence\n",
    "        for c in range(12,len(grp),12):\n",
    "            tmpDF = hlpDF.corr(min_periods=c)\n",
    "            tmpDF = tmpDF.reindex(index=locations, columns=locations)\n",
    "            \n",
    "            if np.all(tmpDF.isnull()):\n",
    "                break\n",
    "                \n",
    "            corrWeights[period][:,:,cnt-1] = corrWeights[period][:,:,cnt-1] + tmpDF.notnull().applymap(int)\n",
    "            \n",
    "    \n",
    "#for period in corrMats:\n",
    "#    corrMats[period] = np.ma.array(corrMats[period], mask=np.isfinite(corrMats[period]))\n",
    "#    corrMats[period] = np.ma.average(corrMats[period], weights=corrWeights[period], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ap = AffinityPropagation(affinity='precomputed')\n",
    "\n",
    "for period in ['day', 'week', 'month']:\n",
    "    print(ap.fit_predict(corrMats[period].fillna(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ap = AffinityPropagation(affinity='precomputed')\n",
    "\n",
    "overlap_dict = {}\n",
    "\n",
    "for period in ['day', 'week', 'month']:\n",
    "    clusters = ap.fit_predict(corrMats[period].fillna(0))\n",
    "    \n",
    "    cl_dict = defaultdict(list)\n",
    "    for i,location in zip(clusters, corrMats[period].columns):\n",
    "        cl_dict[i].append(dataDF.loc[dataDF['LOCOMS'] == location,'SGB'].values[0])\n",
    "    \n",
    "    for key in cl_dict:\n",
    "        cl_dict[key] = list(set(cl_dict[key]))\n",
    "    \n",
    "    matrixDF = pd.DataFrame(index=range(max(clusters)+1), \n",
    "                            columns=range(max(clusters)+1))\n",
    "    \n",
    "    for i in matrixDF.index:\n",
    "        for j in matrixDF.columns:\n",
    "            matrixDF.loc[i,j] = len(set(cl_dict[i]).intersection(set(cl_dict[j])))\n",
    "    \n",
    "    overlap_dict[period] = matrixDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for period in ['day','week', 'month']:\n",
    "    fig,ax = plt.subplots(figsize=(15,15))\n",
    "\n",
    "    mask = np.zeros_like(overlap_dict[period], dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask,1)] = True\n",
    "\n",
    "    sns.heatmap(overlap_dict[period], ax=ax, square=True, annot=True, annot_kws={'size':15}, cmap='inferno_r', mask=mask, cbar=False)\n",
    "\n",
    "    ax.set_xlabel('Cluster label', fontsize=18)\n",
    "    ax.set_ylabel('Cluster label', fontsize=18)\n",
    "    ax.set_title('Stroomgebied overlaps ({} matching)'.format(period), fontsize=22)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inProj = Proj(init='epsg:28992')\n",
    "outProj = Proj(init='epsg:4326')\n",
    "\n",
    "#folium.PolyLine(locations=[p1, p2], color='blue').add_to(some_map)\n",
    "\n",
    "mapa = folium.Map([52., 6.],\n",
    "                  zoom_start=7,\n",
    "                  tiles='cartodbpositron')\n",
    "\n",
    "features = {'day': folium.FeatureGroup(name='daily aggregation'),\n",
    "            'daylines': folium.FeatureGroup(name='daily aggregation (lines)'),\n",
    "            'week': folium.FeatureGroup(name='weekly aggregation'),\n",
    "            'weeklines': folium.FeatureGroup(name='weekly aggregation (lines)'),\n",
    "            'month': folium.FeatureGroup(name='monthly aggregation'),\n",
    "            'monthlines': folium.FeatureGroup(name='monthly aggregation (lines)'),}\n",
    "\n",
    "for period in ['day','week','month']:\n",
    "    ap = AffinityPropagation(affinity='precomputed')\n",
    "    cl = ap.fit_predict(corrMats[period].fillna(0))\n",
    "    \n",
    "    centers = ap.cluster_centers_indices_\n",
    "    \n",
    "    loc_dict = dict(zip(locations,cl))\n",
    "    \n",
    "    pal = sns.color_palette('hls', len(set(cl))).as_hex()\n",
    "    \n",
    "    for loc,group in dataDF.groupby('LOCOMS'):\n",
    "        if group['X_RD'].nunique() != 1:\n",
    "            continue\n",
    "        x = group['X_RD'].unique()[0]\n",
    "        y = group['Y_RD'].unique()[0]\n",
    "        lon,lat = transform(inProj, outProj, x,y)\n",
    "\n",
    "        myHtml = folium.Html('''<h4>Location {}</h4>\n",
    "                               <ul style=\"list-style-type:disc\">\n",
    "                              <li>Cluster: {}</li>\n",
    "                                </ul>\n",
    "                                '''.format(loc, loc_dict[loc]), script=True)\n",
    "\n",
    "        popup = folium.Popup(myHtml, max_width=2650)\n",
    "\n",
    "        folium.Circle([lat,lon], radius=1000, fill=True, fill_color=pal[loc_dict[loc]], fill_opacity=0.7, \n",
    "                      color=pal[loc_dict[loc]], line_opacity=0.8, popup=popup).add_to(features[period])\n",
    "        \n",
    "        # draw lines to the cluster centers\n",
    "        if list(locations).index(loc) in centers:\n",
    "            continue\n",
    "        else:\n",
    "            cl_center = locations[centers[loc_dict[loc]]]\n",
    "            cx = dataDF.loc[dataDF['LOCOMS'] == cl_center, 'X_RD'].values[0]\n",
    "            cy = dataDF.loc[dataDF['LOCOMS'] == cl_center, 'Y_RD'].values[0]\n",
    "            clon,clat = transform(inProj, outProj, cx,cy)\n",
    "            \n",
    "            p1 = [lat,lon]\n",
    "            p2 = [clat,clon]\n",
    "            folium.PolyLine([p1,p2], color=pal[loc_dict[loc]]).add_to(features['{}lines'.format(period)])\n",
    "\n",
    "    features[period].add_to(mapa)    \n",
    "    features['{}lines'.format(period)].add_to(mapa)    \n",
    "\n",
    "folium.LayerControl().add_to(mapa)\n",
    "\n",
    "mapa.save('clustered_locations.html', close_file=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** find a way to properly weight the correlation (for example by using the number of measurements used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "locations = dataDF['LOCOMS'].unique()\n",
    "parameters = dataDF['PAROMS'].unique()\n",
    "\n",
    "corrMats = {'day':np.zeros((len(locations), len(locations), len(parameters))),\n",
    "            'week':np.zeros((len(locations), len(locations), len(parameters))),\n",
    "            'month':np.zeros((len(locations), len(locations), len(parameters)))}\n",
    "\n",
    "corrWeights = {'day':np.zeros((len(locations), len(locations), len(parameters))),\n",
    "            'week':np.zeros((len(locations), len(locations), len(parameters))),\n",
    "            'month':np.zeros((len(locations), len(locations), len(parameters)))}\n",
    "\n",
    "\n",
    "#no_corrs = {'day':np.zeros((len(locations), len(locations))),\n",
    "#            'week':np.zeros((len(locations), len(locations))),\n",
    "#            'month':np.zeros((len(locations), len(locations)))}\n",
    "\n",
    "freq_dict = {'day': '1D', 'week': '1W', 'month':'1m'}\n",
    "\n",
    "for cnt, par in enumerate(parameters):\n",
    "    sys.stdout.write('\\rDoing {} ({}/{})'.format(par,cnt+1,len(parameters)))\n",
    "    \n",
    "    for period in ['day', 'week', 'month']:\n",
    "        tmpDF = pd.pivot_table(dataDF[dataDF['PAROMS'] == par], \n",
    "                               index='datetime', columns=['LOCOMS'], \n",
    "                               values='WAARDE').groupby(pd.Grouper(freq=freq_dict[period])).mean()\n",
    "        \n",
    "        for i,j in product(enumerate(locations),enumerate(locations)):\n",
    "            if (i[1] not in tmpDF.columns) or (j[1] not in tmpDF.columns):\n",
    "                corrWeights[period][i[0],j[0]] = 0.\n",
    "            else:\n",
    "                corrWeights[period][i[0],j[0]] = len(tmpDF.dropna(subset=[i[1], j[1]])) # np.sum(np.all(tmpDF[[i[1],j[1]]].values,axis=1))\n",
    "        \n",
    "        tmpDF = tmpDF.corr(min_periods=12) # with roughly one measurement a month, they should have at least a year of overlaps\n",
    "        tmpDF = tmpDF.reindex(index=locations, columns=locations)\n",
    "\n",
    "        #no_corrs[period] = no_corrs[period] + tmpDF.notnull().applymap(int)\n",
    "\n",
    "        tmpDF.fillna(0, inplace=True)\n",
    "        tmpDF = tmpDF.abs()\n",
    "        \n",
    "        corrMats[period][:,:,cnt] = tmpDF.values\n",
    "    \n",
    "for period in corrMats:\n",
    "    corrMats[period] = np.average(corrMats[period], axis=2, weights=corrWeights[period])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chlorDF = pd.pivot_table(data=dataDF[dataDF['PAROMS'] == 'chlorofyl-a'], columns='LOCOMS', index='datetime', values='WAARDE')\n",
    "\n",
    "chlorDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(15,15), nrows=3)\n",
    "\n",
    "sns.heatmap(chlorDF.loc['2016', chlorDF.columns[6:12]].resample('1D').mean().T.notnull(), \n",
    "            ax=ax[0], xticklabels=False, cbar=False)\n",
    "\n",
    "ax[0].set_ylabel('')\n",
    "ax[0].set_xlabel('')\n",
    "\n",
    "sns.heatmap(chlorDF.loc['2016', chlorDF.columns[6:12]].resample('1W').mean().T.notnull(), \n",
    "            ax=ax[1], xticklabels=False, cbar=False)\n",
    "\n",
    "ax[1].set_ylabel('')\n",
    "ax[1].set_xlabel('')\n",
    "\n",
    "sns.heatmap(chlorDF.loc['2016', chlorDF.columns[6:12]].resample('1M').mean().T.notnull(), \n",
    "            ax=ax[2], xticklabels=False, cbar=False)\n",
    "\n",
    "ax[2].set_ylabel('')\n",
    "ax[2].set_xlabel('2016', fontsize=20)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling the chlorophyl value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_drop = []\n",
    "\n",
    "for i,col in enumerate(featDF.isnull().sum().sort_values(ascending=False).index):\n",
    "    if col == 'chlorofyl-a':\n",
    "        continue\n",
    "    to_drop.append(col)\n",
    "    print('With {} worst columns dropped we get {} samples.'.format(i+1, len(featDF.drop(to_drop, axis=1).dropna())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop three worst columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_drop = list(featDF.isnull().sum().sort_values(ascending=False).index[:3])\n",
    "featDF.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(featDF.shape)\n",
    "featDF.dropna(inplace=True)\n",
    "print(featDF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = featDF.drop('chlorofyl-a', axis=1).values\n",
    "y = featDF['chlorofyl-a'].values\n",
    "\n",
    "trainX, testX, trainY, testY = train_test_split(X,y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "et = ExtraTreesRegressor(n_estimators=150)\n",
    "et.fit(trainX,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "pred = et.predict(testX)\n",
    "\n",
    "def get_relative_error(true,pred):\n",
    "    return 1 - np.sqrt(np.mean(((true-pred)/true)**2))\n",
    "\n",
    "print('The mean accuracy is {0:.2f}%'.format(get_relative_error(testY, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geospatial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myGDF = gpd.read_file('/media/ondrej/500/ds_data/rws_datalab/Waterlichamen/stroomgebied_districten_v.shp')\n",
    "\n",
    "print(myGDF.crs)\n",
    "print(myGDF.shape)\n",
    "\n",
    "myGDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,15))\n",
    "\n",
    "myGDF.plot(ax=ax, linewidth=3, edgecolor='black', cmap='Set1')\n",
    "#for poly in myGDF['geometry']:\n",
    "#    gpd.plotting.plot_dataframe(ax, poly, linewidth=2, edgecolor='grey')\n",
    "#\n",
    "#gpd.plotting.plot_dataframe(myGDF,ax=ax, style_kwds={'linewidth':3})\n",
    "\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "ax.tick_params(size=0) #visible=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling locations based on the big locations - HIGHLY UNFINISHED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataDF['LOCOMS'].value_counts()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#eijsDF = pd.DataFrame(index=pd.date_range(start=dataDF.loc[dataDF['LOCOMS'] == 'Eijsden ponton','datetime'].min(), \n",
    "#                                          end=dataDF.loc[dataDF['LOCOMS'] == 'Eijsden ponton','datetime'].max()))\n",
    "\n",
    "eijsDF = None\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "tot_pars = dataDF.loc[(dataDF['LOCOMS'] == 'Eijsden ponton') & \n",
    "                      (dataDF['BGC'].isnull()), 'PAROMS'].nunique()\n",
    "\n",
    "for par,grp in dataDF[(dataDF['LOCOMS'] == 'Eijsden ponton') & \n",
    "                      (dataDF['BGC'].isnull())].groupby('PAROMS'):\n",
    "    \n",
    "    if len(grp) == 0:\n",
    "        continue\n",
    "    \n",
    "    cnt += 1\n",
    "    \n",
    "    sys.stdout.write('\\rDoing {} ({}/{})'.format(par,cnt,tot_pars))\n",
    "    \n",
    "    if grp['EHD'].nunique() == 1 and grp['HDH'].nunique() == 1:\n",
    "        tmpDF = pd.DataFrame(grp.set_index('datetime')['WAARDE'])\n",
    "        tmpDF.columns = [par]\n",
    "    else:\n",
    "        targ_unit = list(grp['EHD'].value_counts().index)[0]\n",
    "        tmpDF = grp[grp['EHD'] == targ_unit]\n",
    "        if tmpDF['HDH'].nunique() == 1:\n",
    "            tmpDF = tmpDF[['WAARDE','datetime']].set_index('datetime').rename(columns={'WAARDE': par})\n",
    "        else:\n",
    "            tmpDF = pd.pivot_table(tmpDF, columns='HDH', index='datetime', values='WAARDE')\n",
    "            tmpDF.rename(columns=dict(zip(tmpDF.columns,['{}_{}'.format(par,hdh) for hdh in tmpDF.columns])), inplace=True)\n",
    "            \n",
    "    if eijsDF is not None:\n",
    "        eijsDF = pd.merge(eijsDF,tmpDF, left_index=True, right_index=True, how='outer')\n",
    "    else:\n",
    "        eijsDF = tmpDF.copy()\n",
    "\n",
    "eijsDF.head()\n",
    "\n",
    "#pd.pivot_table(dataDF[(dataDF['LOCOMS'] == 'Eijsden ponton') & \n",
    "#                               (dataDF['BGC'].isnull())], columns='PAROMS', index='datetime', values='WAARDE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filteredDF = dataDF[(dataDF['LOCOMS'] == 'Eijsden ponton') & \n",
    "                               (dataDF['BGC'].isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "15*21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.pivot_table(dataDF[(dataDF['LOCOMS'] == 'Eijsden ponton') & \n",
    "                               (dataDF['BGC'].isnull())], columns=['PAROMS','EHD'], index='datetime', values='WAARDE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
